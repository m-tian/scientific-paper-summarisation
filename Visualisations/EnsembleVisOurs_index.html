<html>
<head>

	<title>Paper</title>
	<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">

	<style type="text/css">

	html {
		font-family: 'Source Code Pro', monospace;
	}

	.text {
		margin-bottom: 50px;
	}

	.container {
		text-align: center;
		margin-left: auto;
		margin-right: auto;
		max-width: 70%;
	}
	</style>

</head>

<body>
<div class="container">
<div id="title" class="text"><h1>a supervised approach to extractive summarisation of scientific papers</h1></div><div id="gold" class="text"><h2>Human Written Summary</h2><hr><br><p>we developed a supervised learning algorithm to summarise scientific papers that significantly outperforms baseline methods .<br><br>we have assembled a large dataset , suitable for neural methods , far larger than anything that has been prepared before .<br><br>we have introduced a method to extend the training data and show that this improves performance .<br><br>we have introduced a new feature which exploits the abstract of a paper , taking inspiration from other work in the field of automatic summarisation .<br><br></p></div><div id="paper" class="text"><h2>Full Paper</h2><hr><br><br><br><h3>MAIN-TITLE</h3><p>a supervised approach to extractive summarisation of scientific papers<br><br></p><br><br><h3>KEYPHRASES</h3><p>automatic summarisation machine learning neural networks feature engineering extractive summarisation<br><br></p><br><br><h3>INTRODUCTION</h3><p><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspautomatic summarisation is the task of reducing a document to its main points . </span><br><br>there are two streams of summarisation approaches : extractive summarisation , which copies parts of a document ( often whole sentences ) to form a summary , and abstractive summarisation , which reads a document and then generates a summary from it , which can contain phrases not appearing in the document .<br><br>abstractive summarisation is the more difficult task , but useful for domains where sentences taken out of context are not a good basis for forming a grammatical and coherent summary , like novels .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbsphere , we are concerned with summarising scientific publications . </span><br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspsince scientific publications are a technical domain with fairly regular and explicit language , we opt for the task of extractive summarisation . </span><br><br>although there has been work on summarisation of scientific publications before , existing datasets are very small , consisting of tens of documents ( kupiec etal , 1995 ; visser and wieling , 2009 ) .<br><br>such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding , usually trained on many thousands of documents ( rush etal , 2015 ; cheng and lapata , 2016 ; chopra etal , 2016 ; see etal , 2017 ) .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspin this paper , we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation . </span><br><br>it consists of more than 10k documents and can easily be extended automatically to an additional 26 domains .<br><br>the dataset is created by exploiting an existing resource , sciencedirect,1 where many journals require authors to submit highlight statements along with their manuscripts .<br><br>using such highlight statements as gold statements has been proven a good gold standard for news documents ( nallapati etal , 2016a ) .<br><br>this new dataset offers many exciting research challenges , such how best to encode very large technical documents , which are largely ignored by current research .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspin more detail , our contributions are as follows : we introduce a new dataset for summarisation of scientific publications consisting of over 10k documents . </span><br><br>following the approach of ( hermann etal , 2015 ) in the news domain , we introduce a method , highlightrouge , which can be used to automatically extend this dataset and show empirically that this improves summarisation performance .<br><br>taking inspiration from previous work in summarising scientific literature ( kupiec etal , 1995 ; saggion etal , 2016 ) , we introduce a metric we use as a feature , abstractrouge , which can be used to extract summaries by exploiting the abstract of a paper .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspwe benchmark several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement , which contribute most to the overall score . </span><br><br>we compare our best performing system to several well-established baseline methods , some of which use more elaborate methods to model the global context than we do , and show that our best performing model outperforms them on this extractive summarisation task by a considerable margin .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspwe analyse to what degree different sections in scientific papers contribute to a summary . </span><br><br>we expect the research documented in this paper to be relevant beyond the document summarisation community , for other tasks in the space of automatically understand scientific publications , such as keyphrase extraction ( kim etal , 2010 ; sterckx etal , 2016 ; augenstein etal , 2017 ; augenstein and sgaard , 2017 ) , semantic relation extraction ( gupta and manning , 2011 ; marsi and ozt urk , 2015 ) or topic classification of scientific articles ( o s eaghdha and teufel , 2014 ) .<br><br>we release a novel dataset for extractive summarisation comprised of 10148 computer science publications.2 publications were obtained from sciencedirect , where publications are grouped into 27 domains , computer science being one of them .<br><br>as such , the dataset could easily be extended to more domains .<br><br>an example document is shown in table 1 .<br><br>each paper in this dataset is guaranteed to have a title , abstract , author written highlight statements and author defined keywords .<br><br>the highlight statements are sentences that should effectively convey the main takeaway of each paper and are a good gold summary , while the keyphrases are the key topics of the paper .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspboth abstract and highlights can be thought of as a summary of a paper . </span><br><br>since highlight statements , unlike sentences in the abstract , generally do not have dependencies between them , we opt to use those as gold summary statements for developing our summarisation models , following hermann etal ( 2015 ) ; nallapati etal ( 2016b ) in their approaches to news summarisation .<br><br>as shown by cao etal ( 2015 ) , sentences can be good summaries even when taken out of the context of the surrounding sentences .<br><br>most of the highlights have this characteristic , not relying on any previous or subsequent sentences to make sense .<br><br>consequently , we frame the extractive summarisation task here as a binary sentence classification task , where we assign each sentence in a document a label y 0 , 1 .<br><br>our training data is therefore a list of sentences , sentence features to encode context and a label all stored in a randomly ordered list .<br><br>we used the 10k papers to create two different datasets : cspubsum and cspubsumext where cspubsumext is cspubsum extended with highlightrouge .<br><br>the number of training items for each is given in table 2 .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspcspubsum this datasets positive examples are the highlight statements of each paper . </span><br><br>there are an equal number of negative examples which are sampled randomly from the bottom 10 % of sentences which are the worst summaries for their paper , measured with rouge-l ( see below ) , resulting in 85490 training instances .<br><br>cspubsum test is formed of 150 full papers rather than a randomly ordered list of training sentences .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthese are used to measure the summary quality of each summariser , not the accuracy of the trained models . </span><br><br>cspubsumext the cspubsum dataset has two drawbacks : 1 ) it is an order of magnitude behind comparable large summarisation datasets ( hermann etal , 2015 ; nallapati etal , 2016b ) ; 2 ) it does not have labels for sentences in the context of the main body of the paper .<br><br>we generate additional training examples for each paper with highlightrouge ( see next section ) , which finds sentences that are similar to the highlights .<br><br>this results in 263k instances for cspubsumext train and 132k instances for cspubsumext test .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspcspubsumext test is used to test the accuracy of trained models . </span><br><br>the trained models are then used in summarisers whose quality is tested on cspubsum test with the rouge-l metric ( see below ) .<br><br>rouge metrics are evaluation metrics for summarisation which correspond well to human judgements of good summaries ( lin , 2004 ) .<br><br>we elect to use rouge-l , inline with other research into summarisation of scientific articles ( cohan and goharian , 2015 ; jaidka etal , 2016 ) .<br><br>highlightrouge is a method used to generate additional training data for this dataset , using a similar approach to ( hermann etal , 2015 ) .<br><br>as input it takes a gold summary and body of text and finds the sentences within that text which give the best rouge-l score in relation to the highlights , like an oracle summariser would do .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthese sentences represent the ideal sentences to extract from each paper for an extractive summary . </span><br><br>we select the top 20 sentences which give the highest rouge-l score with the highlights for each paper as positive instances and combine these with the highlights to give the positive examples for each paper .<br><br>an equal number of negative instances are sampled from the lowest scored sentences to match .<br><br>when generating data using highlightrouge , no sentences from the abstracts of any papers were included as training examples .<br><br>this is because the abstract is already a summary ; our goal is to extract salient sentences from the main paper to supplement the abstract , not from the preexisting summary .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspabstractrouge is used as a feature for summarisation . </span><br><br>it is a metric presented by this work which exploits the known structure of a paper by making use of the abstract , a preexisting summary .<br><br>the idea of abstractrouge is that sentences which are good summaries of the abstract are also likely to be good summaries of the highlights .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthe abstractrouge score of a sentence is simply the rouge-l score of that sentence and the abstract . </span><br><br>the intuition of comparing sentences to the abstract is one often used in summarising scientific literature , eg ( saggion etal , 2016 ; kupiec etal , 1995 ) , however these authors generally encode sentences and abstract as tf-idf vectors , then compare them , rather than directly comparing them with an evaluation metric .<br><br>while this may seem somewhat like cheating , all scientific papers are guaranteed to have an abstract so it makes sense to exploit it as much as possible .<br><br></p><br><br><h3>METHOD</h3><p>we encode each sentence in two different ways : as their mean averaged word embeddings and as their recurrent neural network ( rnn ) encoding .<br><br>as the sentences in our dataset are randomly ordered , there is no readily available context for each sentence from surrounding sentences ( taking this into account is a potential future development ) .<br><br>to provide local and global context , a set of 8 features are used for each sentence which are described below .<br><br>these contextual features contribute to achieving the best performances .<br><br>some recent work in summarisation uses as many as 30 features ( dlikman and last , 2016 ; litvak etal , 2016 ) .<br><br>we choose only a minimal set of features to focus more on learning from raw data than on feature engineering , although this could potentially further improve results .<br><br>abstractrouge a new metric presented by this work , described in section 3.2 .<br><br>location authors such as kavila and radhika ( 2015 ) only chose summary sentences from the abstract , introduction or conclusion , thinking these more salient to summaries ; and we show that certain sections within a paper are more relevant to summaries than others ( see section 5.1 ) .<br><br>therefore we assign sentences an integer location for 7 different sections : highlight , abstract , introduction , results / discussion / analysis , method , conclusion , all else.3 location features have been used in other ways in previous work on summarising scientific literature ; visser and wieling ( 2009 ) extract sentence location features based on the headings they occurred beneath while teufel and moens ( 2002 ) divide the paper into 20 equal parts and assign each sentence a location based on which segment it occurred in - an attempt to capture distinct zones of the paper .<br><br>numeric count is the number of numbers in a sentence , based on the intuition that sentences containing heavy maths are unlikely to be good summaries when taken out of context .<br><br>title score in visser and wieling ( 2009 ) and teufel and moens ( 2002 ) s work on summarising scientific papers , one of the features used is title score .<br><br>our feature differs slightly from visser and wieling ( 2009 ) in that we only use the main paper title whereas visser and wieling ( 2009 ) use all section headings .<br><br>to calculate this feature , the non-stopwords that each sentence contains which overlap with the title of the paper are counted .<br><br>keyphrase score authors such as sparck jones ( 2007 ) refer to the keyphrase score as a useful summarisation feature .<br><br>the feature uses author defined keywords and counts how many of these keywords a sentence contains , the idea being that important sentences will contain more keywords .<br><br>tf-idf term frequency , inverse document frequency ( tf-idf ) is a measure of how relevant a word is to a document ( ramos etal , 2003 ) .<br><br>it takes into account the frequency of a word in the current document and the frequency of that word in a background corpus of documents ; if a word is frequent in a document but infrequent in a corpus it is likely to be important to that document .<br><br>tfidf was calculated for each word in the sentence , and averaged over the sentence to give a tf-idf score for the sentence .<br><br>stopwords were ignored .<br><br>document tf-idf document tf-idf calculates the same metric as tf-idf , but uses the count of words in a sentence as the term frequency and count of words in the rest of the paper as the background corpus .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthis gives a representation of how important a word is in a sentence in relation to the rest of the document . </span><br><br>sentence length teufel etal ( 2002 ) created a binary feature for if a sentence was longer than a threshold .<br><br>we simply include the length of the sentence as a feature ; an attempt to capture the intuition that short sentences are very unlikely to be good summaries because they can not possibly convey as much information as longer sentences .<br><br>models detailed in this section could take any combination of four possible inputs , and are named accordingly : s : the sentence encoded with an rnn .<br><br>a : a vector representation of the abstract of a paper , created by averaging the word vectors of every non-stopword word in the abstract .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspsince an abstract is already a summary , this gives a good sense of relevance . </span><br><br>it is another way of taking the abstract into consideration by using neural methods as opposed to a feature .<br><br>a future development is to encode this with an rnn .<br><br>f : the 8 features listed in section 4.1 .<br><br>word2vec : the sentence represented by taking the average of every non-stopword word vector in the sentence .<br><br>models containing net use a neural network with one or multiple hidden layers .<br><br>models ending with ens use an ensemble .<br><br>all non-linearity functions are rectified linear units ( relus ) , chosen for their faster training time and recent popularity ( krizhevsky etal , 2012 ) .<br><br>single feature models the simplest class of summarisers use a single feature from section 4.1 ( sentence length , numeric count and section are excluded due to lack of granularity when sorting by these ) .<br><br>features only : fnet a single layer neural net to classify each sentence based on all of the 8 features given in section 4.1 .<br><br>a future development is to try this with other classification algorithms .<br><br>word vector models : word2vec and word2vecaf both single layer networks .<br><br>word2vec takes as input the sentence represented as an averaged word vector of 100 numbers.4 word2vecaf takes the sentence average vector , abstract average vector and handcrafted features , giving a 208-dimensional vector for classification .<br><br>lstm-rnn method : snet takes as input the ordered words of the sentence represented as 100-dimensional vectors and feeds them through a bi-directional rnn with long-short term memory ( lstm , hochreiter and schmidhuber ( 1997 ) ) cells , with 128 hidden units and dropout to prevent overfitting .<br><br>dropout probability was set to 0.5 which is thought to be near optimal for many tasks ( srivastava etal , 2014 ) .<br><br>output from the forwards and backwards lstms is concatenated and projected into two classes.5 lstm and features : sfnet sfnet processes the sentence with an lstm as in the previous paragraph and passes the output through a fully connected layer with dropout .<br><br>the handcrafted features are treated as separate inputs to the network and are passed through a fully connected layer .<br><br>the outputs of the lstm and features hidden layer are then concatenated and projected into two classes .<br><br>safnet safnet , shown in figure 1 is the most involved architecture presented in this paper , which further to sfnet also encodes the abstract .<br><br>ensemble methods : saf+f and s+f ensemblers the two ensemble methods use a weighted average of the output of two different models : psummary = s1 ( 1 c ) + s2 ( 1 + c ) 2 where s1 is the output of the first summariser , s2 is the output of the second and c is a hyperparameter .<br><br>saf+f ensembler uses safnet as as s1 and fnet as s2 .<br><br>s+f ensembler uses snet as s1 and fnet as s2 .<br><br></p><br><br><h3>RESULTS AND ANALYSIS</h3><p>a straight-forward heuristic way of obtaining a summary automatically would be to identify which sections of a paper generally represent good summaries and take those sections as a summary of the paper .<br><br>this is precisely what kavila and radhika ( 2015 ) do , constructing summaries only from the abstract , introduction and conclusion .<br><br>this approach works from the intuition that certain sections are more relevant to summaries .<br><br>to understand how much each section contributes to a gold summary , we compute the rouge-l score of each sentence compared to the gold summary and average sentence-level rouge-l scores by section .<br><br>rouge-type metrics are not the only metrics which we can use to determine how relevant a sentence is to a summary .<br><br>throughout the data , there are approximately 2000 occurrences of authors directly copying sentences from within the main text to use as highlight statements .<br><br>by recording from which sections of the paper these sentences came , we can determine from which sections authors most frequently copy sentences to the highlights , so may be the most relevant to a summary .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthis is referred to as the copy/paste score in this paper . </span><br><br>figure 2 shows the average rouge score for each section over all papers , and the normalised copy/paste score .<br><br>the title has the highest rouge score in relation to the gold summary , which is intuitive as the aim of a title is to convey information about the research in a single line .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspa surprising result is that the introduction has the third-lowest rouge score in relation to the highlights . </span><br><br>our hypothesis was that the introduction would be ranked highest after the abstract and title because it is designed to give the reader a basic background of the problem .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspindeed , the introduction has the second highest copy/paste score of all sections . </span><br><br>the reason the introduction has a low rouge score but high copy/paste score is likely due to its length .<br><br>the introduction tends to be longer ( average length of 72.1 sentences ) than other sections , but still of a relatively simple level compared to the method ( average length of 41.6 sentences ) , thus has more potential sentences for an author to use in highlights , giving the high copy/paste score .<br><br>however it would also have more sentences which are not good summaries and thus reduce the overall average rouge score of the introduction .<br><br>hence , although some sections are slightly more likely to contain good summary sentences , and assuming that we do not take summary sentences from the abstract which is already a summary , then figure 2 suggests that there is no definitive section from which summary sentences should be extracted .<br><br>figure 3 shows comparisons of the best model we developed to well-established external baseline methods .<br><br>our model can be seen to significantly outperform these methods , including graph-based methods which take account of global context : lexrank ( radev , 2004 ) and textrank ( mihalcea and tarau , 2004 ) ; probabilistic methods in klsum ( kl divergence summariser , haghighi and vanderwende ( 2009 ) ) ; methods based on singular value decomposition with lsa ( latent semantic analysis , steinberger and jezek ( 2004 ) ) ; and simple methods based on counting in sumbasic ( vanderwende etal , 2007 ) .<br><br>this is an encouraging result showing that our methods that combine neural sentence encoding and simple features for representing the global context and positional information are very effective for modelling an extractive summarisation problem .<br><br>figure 4 shows the performance of all models developed in this work measured in terms of accuracy and rouge-l on cspubsumext test and cspubsum test , respectively .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbsparchitectures which use a combination of sentence encoding and additional features performed best by both measures . </span><br><br>the lstm encoding on its own outperforms models based on averaged word embeddings by 6.7 % accuracy and 2.1 rouge points .<br><br>this shows that the ordering of words in a sentence clearly makes a difference in deciding if that sentence is a summary sentence .<br><br>this is a particularly interesting result as it shows that encoding a sentence with an rnn is superior to simple arithmetic , and provides an alternative to the recursive autoencoder proposed by ( socher etal , 2011 ) which performed worse than vector addition .<br><br>another interesting result is that the highest accuracy on cspubsumext test did not translate into the best rouge score on cspubsum test , although they are strongly correlated ( pearson correlation , r=0.8738 ) .<br><br>safnet achieved the highest accuracy on cspubsumext test , however was worse than the abstractrouge summariser on cspubsum test .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthis is most likely due to imperfections in the training data . </span><br><br>a small fraction of sentences in the training data are mislabelled due to bad examples in the highlights which are exacerbated by the highlightrouge method .<br><br>this leads to confusion for the summarisers capable of learning complex enough representations to classify the mislabelled data correctly .<br><br>we manually examined 100 sentences from cspubsumext which were incorrectly classified by safnet .<br><br>out of those , 37 are mislabelled examples .<br><br>the primary cause of false positives was lack of context ( 16 / 50 sentences ) and long range dependency ( 10 / 50 sentences ) .<br><br>other important causes of false positives were mislabelled data ( 12 / 50 sentences ) and a failure to recognise that mathematically intense sentences are not good summaries ( 7 / 50 sentences ) .<br><br>lack of context is when sentences require information from the sentences immediately before them to make sense .<br><br>for example , the sentence the performance of such systems is commonly evaluated using the data in the matrix is classified as positive but does not make sense out of context as it is not clear what systems the sentence is referring to .<br><br>a long-range dependency is when sentences refer to an entity that is described elsewhere in the paper , eg sentences referring to figures .<br><br>these are more likely to be classified as summary statements when using models trained on automatically generated training data with highlightrouge , because they have a large overlap with the summary .<br><br>the primary cause of false negatives was mislabelled data ( 25 / 50 sentences ) and failure to recognise an entailment , observation or conclusion ( 20 / 50 sentences ) .<br><br>mislabelled data is usually caused by the presence of some sentences in the highlights which are of the form we set m=10 in this approach , which are not clear without context .<br><br>such sentences should only be labelled as positive if they are part of multi-line summaries , which is difficult to determine automatically .<br><br>failure to recognise an entailment , observation or conclusion is where a sentence has the form entity x seems to have a very small effect on y for example , but the summariser has not learnt that this information is useful for a summary , possibly because it was occluded by mislabelled data .<br><br>safnet and sfnet achieve high accuracy on the automatically generated cspubsumext test dataset , though a lower rouge score than other simpler methods such as fnet on cspubsum test .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthis is likely due to overfitting , which our simpler summarisation models are less prone to . </span><br><br>one option to solve this would be to manually improve the cspubsumext labels , the other to change the form of the training data .<br><br>rather than using a randomised list of sentences and trying to learn objectively good summaries ( cao etal , 2015 ) , each training example could be all the sentences in order from a paper , classified as either summary or not summary .<br><br>the best summary sentences from within the paper would then be chosen using highlightrouge and used as training data , and an approach similar to nallapati etal ( 2016a ) could be used to read the whole paper sequentially and solve the issue of long-range dependencies and context .<br><br>the issue faced by safnet does not affect the ensemble methods so much as their predictions are weighted by a hyperparameter tuned with cspubsum test rather than cspubsumext .<br><br>ensemblers ensure good performance on both test sets as the two models are adapted to perform better on different examples .<br><br>in summary , our model performances show that : reading a sentence sequentially is superior to averaging its word vectors , simple features that model global context and positional information are very effective and a high accuracy on an automatically generated test set does not guarantee a high rouge-l score on a gold test set , although they are correlated .<br><br>this is most likely caused by models overfitting data that has a small but significant proportion of mislabelled examples as a byproduct of being generated automatically .<br><br>this work used a method similar to hermann etal ( 2015 ) to generate extra training data ( section 3.1 ) .<br><br>figure 5 compares three models trained on cspubsumext train and the same models trained on cspubsum train ( the feature of which section the example appeared in was removed to do this ) .<br><br>the fnet summariser and sfnet suffer statistically significant ( p = 0.0147 and p < 0.0001 ) drops in performance from using the unexpanded dataset , although interestingly safnet does not , suggesting it is a more stable model than the other two .<br><br>these drops in performance however show that using the method we have described to increase the amount of available training data does improves model performance for summarisation .<br><br>this work suggested use of the abstractrouge metric as a feature ( section 3.2 ) .<br><br>figure 6 compares the performance of 3 models trained with and without it .<br><br>this shows two things : the abstractrouge metric does improve performance for summarisation techniques based only on feature engineering ; and learning a representation of the sentence directly from the raw text as is done in safnet and sfnet as well as learning from features results in a far more stable model .<br><br>this model is still able to make good predictions even if abstractrouge is not available for training , meaning the models need not rely on the presence of an abstract .<br><br></p><br><br><h3>RELATED WORK</h3><p>datasets datasets for extractive summarisation often emerged as part of evaluation campaigns for summarisation of news , organised by the document understanding conference ( duc ) , and the text analysis conference ( tac ) .<br><br>duc proposed single-document summarisation ( harman and over , 2002 ) , whereas tac datasets are for multi-document summarisation ( dang and owczarzak , 2008 , 2009 ) .<br><br>all of the datasets contain roughly 500 documents .<br><br>the largest summarisation dataset ( 1 million documents ) to date is the dailymail/cnn dataset ( hermann etal , 2015 ) , first used for single-document abstractive summarisation by ( nallapati etal , 2016b ) , enabling research on data-intensive sequence encoding methods .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspexisting datasets for summarisation of scientific documents of which we are aware are small . </span><br><br>kupiec etal ( 1995 ) used only 21 publications and cl-scisumm 20176 contains 30 publications .<br><br>ronzano and saggion ( 2016 ) used a set of 40 papers , kupiec etal ( 1995 ) used 21 and visser and wieling ( 2009 ) used only 9 papers .<br><br>the largest known scientific paper dataset was used by teufel and moens ( 2002 ) who used a subset of 80 papers from a larger corpus of 260 articles .<br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspthe dataset we introduce in this paper is , to our knowledge , the only large dataset for extractive summarisation of scientific publications . </span><br><br>the size of the dataset enables training of data-intensive neural methods and also offers exciting research challenges centered around how to encode very large documents .<br><br>extractive summarisation methods early work on extractive summarisation focuses exclusively on easy to compute statistics , eg word frequency ( luhn , 1958 ) , location in the document ( baxendale , 1958 ) , and tf-idf ( salton etal , 1996 ) .<br><br>supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular ( kupiec etal , 1995 ) .<br><br>exploration of more cues such as sentence position ( yang etal , 2017 ) , sentence length ( radev etal , 2004 ) , words in the title , presence of proper nouns , word frequency ( nenkova etal , 2006 ) and event cues ( filatova and hatzivassiloglou , 2004 ) followed .<br><br>recent approaches to extractive summarisation have mostly focused on neural approaches , based on bag of word embeddings approaches ( kobayashi etal , 2015 ; yogatama etal , 2015 ) or encoding whole documents with cnns and/or rnns ( cheng and lapata , 2016 ) .<br><br>in our setting , since the documents are very large , it is computationally challenging to read a whole publication with a ( possibly hierarchical ) neural sequence encoder .<br><br>in this work , we therefore opt to only encode the target sequence with an rnn and the global context with simpler features .<br><br>we leave fully neural approaches to encoding publications to future work .<br><br></p><br><br><h3>CONCLUSION</h3><p><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspin this paper , we have introduced a new dataset for summarisation of computer science publications , which is substantially larger than comparable existing datasets , by exploiting an existing resource . </span><br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspwe showed the performance of several extractive summarisation models on the dataset that encode sentences , global context and position , which significantly outperform well-established summarisation methods . </span><br><br><span style="background-color:hsla(0, 100%, 50%, 0.5)">&nbspwe introduced a new metric , abstractrouge , which we show increases summarisation performance . </span><br><br>finally , we show how the dataset can be extended automatically , which further increases performance .<br><br>remaining challenges are to better model the global context of a summary statement and to better capture crosssentence dependencies .<br><br></p></div></body></html>